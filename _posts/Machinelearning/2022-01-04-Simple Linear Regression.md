---
title : "⚙ 2. 단순 선형회귀"

categories:
    - Machinelearning
tags:
    - [AI, Regression, Cost]

toc : true
toc_sticky : true
use_math : true

date: 2022-01-04
last_modified_at: 2022-01-04
---  

* * *
🔨 이번 포스팅에서는 선형회귀에 대해서 알아보자.

## 1. Regression

🔨 어떤 데이터가 너무 크거나 작아도 모든 데이터들은 전체의 평균으로 되돌아가려는 (회귀하려는) 특성이 있음

### 1.1. Linear Regression
* * *

🔨 어떤 값들이 선형적인 증가 / 감소 관계에 있을 떄 이 관계를 해석하는 것
- 즉, Linear Regression은 데이터를 가장 잘 나타내는 <b>직선의 방정식</b> (기울기 / y절편) 을 찾는 것이다.

### 1.2. Hypothesis
* * *

🔨 Hypothesis - <b>가설함수</b>  
<center>$H(x) = Wx + b$</center>

- 가설 : 이 직선이 데이터의 특징을 가장 잘 나타내는 직선이다!!
- 이제 이 가설이 데이터를 가장 잘 나타내도록 기울기(W)와 y절편(b)을 찾아야 한다.
- ($H(x_i) - y_i$ ) : 가설과 실제 데이터와의 차이. 오차 / Loss / Error 라고 한다.

### 1.3. Cost
* * *

🔨 Cost - <b>비용함수</b>  
<center>$Cost(W,b) = \frac{1}{m} \sum_{i=1}^m (H(x_i) - y_i)^2$</center>

- <b>오차 제곱의 평균</b>
- 위의 비용함수에 $x_i$ 값과 $y_i$ 값을 대입하면 비용이 구해짐
- 단순히 cost의 총합을 최소화하려고 하면 양수 / 음수의 문제가 발생한다. 따라서 cost를 제곱해서 이 값을 최소화한다.

### 1.4. ⭐목표
* * *

🔨 Cost를 최소화하는 기울기와 y 절편을 찾는 것이 선형회귀의 목표이다!!

***

🔨 이번 포스팅에서는 가설함수, 비용함수와 선형회귀의 목표에 대해서 알아보았다. 각 함수들의 형태를 아는 것이 중요하다고 생각한다.  

🔨 다음 포스팅에서는 실제 코드로 가설함수와 비용함수를 구현해보자.